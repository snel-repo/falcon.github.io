- title: H1_7d
  slug: h1
  image: assets/h1.png
  dandi: "[DANDI repo](https://dandiarchive.org/dandiset/000954)"
  notebook: https://github.com/snel-repo/stability-benchmark/blob/main/data_demos/h1.ipynb
  text: |
    H1 contains open-loop calibration data collected in one human participant for a reach and grasp BCI experiment.  The participant watches a virtual arm perform a series of movements and attempts to match these movements. The participant cannot execute these movements fully due to high level spinal cord injury, and for these datasets, their native limbs were completely at rest. The virtual arm behavior is phasic and pre-specified, such that movement are split into translation, orientation, and grasp phases. Neural activity is collected from Utah arrays implanted in the arm and hand areas of motor cortex. This data is provided by the Rehab Neural Engineering Labs at University of Pittsburgh.

    **Why H1?** Motor population activity represent a breadth of movement variables that are accessible through linear decoding. This richness has been used in microelectrode-based BCIs to enable high-dimensional arm and hand control first demonstrated over a decade ago. The saliency of movement-related information has been a hallmark motivation for microelectrode-based BCIs. Nonetheless, two challenges remain:

    - High dimensional control remains a burden to calibrate, with this particular experimental protocol demanding several minutes of calibration.
    - The identified decoders do not provide stable control, such that experimenters routinely calibrate new decoders in each experimental session despite the burden.

    Providing methods to improve the efficiency of calibration in novel sessions would thus greatly improve the practical viability of high dimensional neuroprosthetic motor control from spikes.
- title: H2_writing
  slug: h2
  image: assets/h2_figure.png
  dandi: "[DANDI repo](https://dandiarchive.org/dandiset/000950)"
  notebook: "https://github.com/snel-repo/falcon-challenge/blob/main/data_demos/h2.ipynb"
  text: |
    H2 contains data from a human iBCI partipant performing a handwriting task. On each trial, the participant is given a cue to copy by attempting to write each letter in the prompt. Neural activity is collected from two Utah arrays implanted in the hand "knob" area of cortex. This data is provided by Chaofei Fan at Stanford University. 

    **Why H2?** The H2 dataset falls in the domain of brain-to-text BCI, which aim to restore communication capabilities. Decoders for this task will need to be able to accurately predict the intended character as well as determine when that character was intended to be written, as the task is fully self-paced. This task is therefore not amenable to traditional linear decoders. Additionally, decoding may make use of large language models, allowing a new class of recalibration approaches.
- title: M1_reach
  slug: m1
  image: assets/m1_behavior.png
  dandi: "[DANDI repo](https://dandiarchive.org/dandiset/000941)"
  notebook: https://github.com/snel-repo/falcon-challenge/blob/main/data_demos/m1.ipynb
  text: |
    M1 contains data from a monkey performing a center-out reach-and-grasp task. The dataset comprises neural activity recordings from a rhesus monkey equipped with 6 floating microelectrode arrays, each with 16 channels. Additionally, intramuscular electromyography (EMG) data is captured from 16 locations in the monkey's right hand and upper extremity muscles. These recordings are obtained while the monkey performs tasks involving reaching, grasping, and manipulating four different objects positioned at eight different locations. The objects include various shapes such as cylinders, buttons, spheres, etc., and the monkey is trained to perform specific manipulation actions for each object type. Trials begin with the monkey interacting with a central cylinder, followed by cues indicating which peripheral object to manipulate. Successful trials involve the monkey completing the required interactions with the cued object within specified time frames. This data is provided by Adam Rouse at University of Kansas.

    **Why M1?** Successfully working with this dataset requires high-accuracy and consistent EMG decoding performance. EMG decoding is particularly challenging as it requires a model to forecast multiple output variables (here, 16 muscles). Moreover, EMG decoding bears relevance to BCI developments aiming to apply functional electrical stimulation (FES) for inducing movement in a user's limb. However, EMG data is costly and difficult to collect as it requires an invasive implant in the muscles, and many BCI users may not have muscular control that can yield high fidelity EMG recordings.

    As a first step, developing methods to improve the stability of EMG decoding would limit the demand to collect new EMG calibration data, thus greatly improving the practical viability of EMG-based neuroprosthetic motor control. Future endeavors may also aim to develop methods for cross-participant transfer to increase the amount of high-quality EMG data available for BCI applications.
- title: M2_finger
  slug: m2
  image: assets/m2_behavior.png
  dandi: "[DANDI repo](https://dandiarchive.org/dandiset/000953)"
  notebook: https://github.com/snel-repo/falcon-challenge/blob/main/data_demos/m2.ipynb
  text: |
    M2 is a dataset of a monkey moving two finger groups independently into targets cued on a screen. The dataset has neural activity from two 64-channel Utah arrays (only 96 channels provided) and finger position data as recorded by the state of the finger manipulandum in which the monkey placed its fingers. The monkey has been well-trained to do this task and the data is collected in the context of controlling finger movements using a BCI. The data is provided by the Chestek lab at University of Michigan.

    **Why M2?** Finger control is a critical aspect of dexterous hand function and is a key target for neuroprosthetic control. M2's continuous finger behavior is a non-prehensile behavior, distinguishing it from the other major class of prehensile, grasping behavior (as in M1). Non-prehensile finger movements provide more controllable dimensions than typical two-dimensional BCI applications (i.e. cursor control), potentially enabling simultaneous control of more effectors (as proposed by [Willsey et al., 24](https://pubmed.ncbi.nlm.nih.gov/38370697/)). As one gains the ability to control more dimensions with a BCI, obtaining training data for all of those control dimensions becomes insurmountable, particularly if frequent decoder retraining is necessary. As such, the ability to predict individuated finger movements over months without collecting explicit training data representing all behaviors will make high-dimensional BCIs more practical. Further, recent work, as in [Nason et al., 21, where this data was collected](https://pubmed.ncbi.nlm.nih.gov/34499856/) and [Shah et al., 23](https://pubmed.ncbi.nlm.nih.gov/37873182/), are identifying linear compositionality in the encoding of different finger behaviors in motor cortex. However, this structure has unclear implications for the stability of decoding finger movements for BCI control.
